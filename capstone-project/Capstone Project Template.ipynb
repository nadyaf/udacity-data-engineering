{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Building Data Warehouse with U.S Immigration data\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project uses the U.S immigration and several satellite datasets to help understand immigration trends in the U.S. The data is aggregated and stored in a data warehouse to be used for analytics, such as: \n",
    "- understanding which airports are the most popular. Knowing this can help, for example, with airport infrastructure improvements\n",
    "- understanding which states are the most popular\n",
    "- how travel trends depend on area demographics\n",
    "- how many students/business travellers are there and to which areas they travel\n",
    "\n",
    "The project follows the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import hashlib\n",
    "import uuid\n",
    "import re\n",
    "import configparser\n",
    "import psycopg2\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project the data is ingested from source files into S3, then cleaned using Spark, saved to parquet and after that loaded into Redshift data warehouse. Data analysts will be able to analyse data in Redshift.\n",
    "The process is as follows:\n",
    "1. Ingest data from source files (parquet, csv) into the raw datalake.\n",
    "2. Read raw data from datalake, clean it and save back to S3 (cleansed layer).\n",
    "3. Build data model from cleaned data and save back to S3 (application layer).\n",
    "4. Load data from application layer into Redshift for analysis using star schema.\n",
    "\n",
    "#### Data Description\n",
    "Currently data is collected from 3 sources:\n",
    "- __US National Tourism and Trade Office__: [link](https://www.trade.gov/national-travel-and-tourism-office)\n",
    " I-94 is the U.S Visitor Arrivals Program which provides a count of visitor arrivals to the United States (with stays of 1-night or more and visiting under certain visa types) to calculate U.S. travel and tourism volume exports. It includes information about a country where a person came from, departure date, arrival state, persons' details (birth year, gender), visa category.\n",
    " This data is in parquet format in *sas_data* folder. Come columns (country, state, port, visa type) use numeric codes. The definitions for these codes are in sas_metadata folder in separate csv file for every column.\n",
    "- __U.S. City Demographic Data__: [link](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "Contains the following data:\n",
    " *City*, *State*, *MedianAge*, *Male Population*, *Female Population*, *Total Population*, *Number of Veterans*, *Foreign-born*, *Average Household Size*, *State Code*, *Race*, *Count*\n",
    "- __Airport Code Table__: [link](https://datahub.io/core/airport-codes#data)\n",
    "Contains the following data: \n",
    " *ident*, *type*, *name*, *elevation_ft*, *continent*, *iso_country*, *iso_region*, *municipality*, *gps_code*, *iata_code*, *local_code*, *coordinates*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "CONFIG_FILE_KEY = 'config.cfg'\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(CONFIG_FILE_KEY)\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config.get('AWS', 'AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config.get('AWS', 'AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATALAKE_RAW_PATH = config.get('S3', 'DATALAKE_RAW_PATH')\n",
    "DATALAKE_CLEAN_PATH = config.get('S3', 'DATALAKE_CLEAN_PATH')\n",
    "DATALAKE_APPLICATION_PATH = config.get('S3', 'DATALAKE_APPLICATION_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read sample data\n",
    "fname = 'immigration_data_sample.csv'\n",
    "df = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Udacity Capstone Project') \\\n",
    "    .master('local[4]') \\\n",
    "    .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "    .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = spark.read.parquet('sas_data/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make sure we can see all columns\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert double data types to integer\n",
    "df_spark = df_spark.withColumn('cicid', F.col('cicid').cast(IntegerType())) \\\n",
    "                    .withColumn('i94yr', F.col('i94yr').cast(IntegerType())) \\\n",
    "                    .withColumn('i94mon', F.col('i94mon').cast(IntegerType())) \\\n",
    "                    .withColumn('i94cit', F.col('i94cit').cast(IntegerType())) \\\n",
    "                    .withColumn('i94res', F.col('i94res').cast(IntegerType())) \\\n",
    "                    .withColumn('arrdate', F.col('arrdate').cast(IntegerType())) \\\n",
    "                    .withColumn('i94mode', F.col('i94mode').cast(IntegerType())) \\\n",
    "                    .withColumn('depdate', F.col('depdate').cast(IntegerType())) \\\n",
    "                    .withColumn('i94bir', F.col('i94bir').cast(IntegerType())) \\\n",
    "                    .withColumn('i94visa', F.col('i94visa').cast(IntegerType())) \\\n",
    "                    .withColumn('count', F.col('count').cast(IntegerType())) \\\n",
    "                    .withColumn('biryear', F.col('biryear').cast(IntegerType())) \\\n",
    "                    .withColumn('admnum', F.col('admnum').cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Save I-94 Immigration data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark.write \\\n",
    "    .partitionBy(\"i94yr\", \"i94mon\") \\\n",
    "    .save(path=DATALAKE_RAW_PATH+'/i94-immigration', source='parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Read U.S. demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographics_data_fname = 'us-cities-demographics.csv'\n",
    "df_demographics_data = spark.read.format(\"csv\") \\\n",
    "  .option(\"sep\", \";\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .load(demographics_data_fname)\n",
    "df_demographics_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace spaces in columns names with underscores\n",
    "df_demographics_data = df_demographics_data.toDF(*(c.replace(' ', '_') for c in df_demographics_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Save U.S demographics data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics_data.coalesce(4).write \\\n",
    "    .partitionBy(\"State_Code\") \\\n",
    "    .save(path=DATALAKE_RAW_PATH+'/us-demographics', source='parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Load Airport codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airport_codes_data_fname = 'airport-codes_csv.csv'\n",
    "df_airport_codes_data = spark.read.format(\"csv\") \\\n",
    "  .option(\"sep\", \",\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .load(airport_codes_data_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Save Airport data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes_data.write \\\n",
    "    .save(path=DATALAKE_RAW_PATH+'/airports', source='parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Load SAS metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "BASE_METADATA_PATH = './sas_metadata'\n",
    "sas_metadata_files = glob.glob(BASE_METADATA_PATH+\"/*.csv\")\n",
    "\n",
    "[os.path.basename(x) for x in sas_metadata_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "metadataSchema = StructType([\n",
    "    StructField(\"Code\", StringType(), False),\n",
    "    StructField(\"Value\", StringType(), False)\n",
    "])\n",
    "\n",
    "sas_metadata_dfs = dict()\n",
    "for fname in [os.path.basename(x) for x in sas_metadata_files]:\n",
    "    print(f\"{BASE_METADATA_PATH}/{fname}, key={fname.split('.')[0]}\")\n",
    "    sas_metadata_dfs[fname.split('.')[0]] = spark.read.format(\"csv\") \\\n",
    "      .option(\"sep\", \"=\")\\\n",
    "      .option(\"header\", \"false\")\\\n",
    "      .load(f'{BASE_METADATA_PATH}/{fname}', schema=metadataSchema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_metadata_dfs['i94addrl'].toPandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Have a look at the data\n",
    "sas_metadata_dfs['i94cntyl'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "These steps will be performed to clean the data:\n",
    "1. Remove the columns that won't be used in the final model\n",
    "2. Check for nulls and remove rows with nulls\n",
    "3. Remove duplicates\n",
    "4. Filter out redundant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Clean Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 1. Remove columns that are not needed\n",
    "columns_to_drop = ['count', 'dtadfile', 'visapost', 'occup','entdepa', 'entdepd', 'entdepu', 'matflag', 'dtaddto', 'insnum', \\\n",
    "                   'airline', 'admnum', 'fltno']\n",
    "df_spark_clean = df_spark.drop(*columns_to_drop)\n",
    "#df_spark_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "is_null_or_nan = F.udf(lambda col: F.isnan(F.col(col)) | F.col(col).isNull(), BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 2. Check for null and missing values\n",
    "df_spark_clean.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df_spark_clean.columns]).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 3. Drop rows with NULLs. Since there's not too many nulls, it won't affect data analysis.\n",
    "df_spark_clean_no_na = df_spark_clean.na.drop(subset=[\"i94mode\", 'i94addr', 'i94bir', 'biryear', 'gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_clean_no_na.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 4. Drop duplciates\n",
    "df_spark_clean_no_na = df_spark_clean_no_na.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 5. Parse dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "According to this http://www.scsug.org/wp-content/uploads/2018/10/Horstman_SCSUG2018_Dating_for_SAS_Programmers.pdf, a SAS date value is stored as the number of days since January 1, 1960. Thus, the date January 1,\n",
    "1960, corresponds to a value of zero. Likewise, January 2, 1960, would be represented as 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "BASE_SAS_DATE = '1960-01-01'\n",
    "df_spark_clean_no_na = df_spark_clean_no_na.withColumn('arrival_date', F.expr(f\"date_add(to_date('{BASE_SAS_DATE}'), arrdate)\")) \\\n",
    "                        .withColumn('departure_date', F.expr(f\"date_add(to_date('{BASE_SAS_DATE}'), depdate)\")) \\\n",
    "                        .drop('arrdate') \\\n",
    "                        .drop('depdate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark_clean_no_na.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Clean U.S. demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check for NULLs and missing values\n",
    "df_demographics_data.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df_demographics_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df_demographics_data_no_na = df_demographics_data.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df_demographics_data_clean = df_demographics_data_no_na.drop_duplicates()\n",
    "df_demographics_data_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Verify the states are correct\n",
    "df_demographics_data_no_na.select('State_Code').distinct().sort('State_Code').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics_data_no_na.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics_data_no_na.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics_data_clean = df_demographics_data_no_na \\\n",
    "    .coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics_data_clean \\\n",
    "    .withColumn(\"state_abbr\", F.col('state_code')) \\\n",
    "    .write \\\n",
    "    .partitionBy(\"state_abbr\") \\\n",
    "    .save(path=DATALAKE_CLEAN_PATH+'/us-demographics', source='parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics_data_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Clean airports data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check for NULLs and missing values\n",
    "df_airport_codes_data.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df_airport_codes_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df_airport_codes_data_no_na = df_airport_codes_data.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df_airport_codes_data_clean = df_airport_codes_data_no_na.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes_data_clean.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Airport data contains airports from all over the world. \n",
    "For this project we're only interested in US airports so let's leave only them, thus reducing the data set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# First, let's see what countries are there\n",
    "df_airport_codes_data_clean.select('iso_country').distinct().sort('iso_country').show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes_data_clean = df_airport_codes_data_clean.filter(F.col('iso_country')=='US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes_data_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Column iso_region is in format \"US-StateCode\". Let's extract state code into a separate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# First, let's have a look at all the state codes\n",
    "df_airport_codes_data_clean.select('iso_region').distinct().sort('iso_region').show(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes_data_clean = df_airport_codes_data_clean \\\n",
    "                                .withColumn(\"state_code\", F.split(F.col(\"iso_region\"),\"-\").getItem(1).alias(\"state_code\")) \\\n",
    "                                .drop('iso_region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes_data_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove 'continent', 'iso_country' columns since we're only using US airports data\n",
    "df_airport_codes_data_clean = df_airport_codes_data_clean.drop(*['continent', 'iso_country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes_data_clean.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Save cleaned airports data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes_data_clean = df_airport_codes_data_clean \\\n",
    "    .withColumn('state', F.col('state_code')) \\\n",
    "    .coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes_data_clean \\\n",
    "    .write \\\n",
    "    .partitionBy(\"state\") \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet(DATALAKE_CLEAN_PATH+'/airports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Clean metadata\n",
    "First, we need to remove extra characters from the values\n",
    "'i94cntyl.csv', 'i94visa.csv', 'i94prtl.csv', 'i94model.csv', 'i94addrl.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_extra_characters(val):\n",
    "    return val.strip(\" '\")\n",
    "\n",
    "clean_extra_characters_udf = F.udf(clean_extra_characters, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Clean countries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_metadata_dfs['i94cntyl'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_countries_df = sas_metadata_dfs['i94cntyl'].withColumn('Code', F.regexp_extract(F.col('Code'), '(\\d+)', 1)) \\\n",
    "                    .withColumn('Value', F.regexp_extract(F.col('Value'), '([a-zA-Z0-9\\.,\\s\\-\\(\\)]+[a-zA-Z0-9\\.\\)])', 1))\n",
    "sas_countries_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Clean airports (from SAS metadata)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_metadata_dfs['i94prtl'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_airports_df = sas_metadata_dfs['i94prtl'].withColumn('Code', F.regexp_extract(F.col('Code'), '([A-Za-z\\d]{2,3}+)', 1)) \\\n",
    "                    .withColumn('Value', F.regexp_extract(F.col('Value'), '([a-zA-Z0-9\\.,\\s\\-\\(\\)]+[a-zA-Z0-9\\.\\)])', 1))\n",
    "sas_airports_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Value column in Airports is in format \"City, State\". Let's split them and place in 2 different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_airports_df = sas_airports_df.select('Code', \\\n",
    "                       F.split(F.col(\"Value\"),\", \").getItem(0).alias(\"City\"), \\\n",
    "                       F.split(F.col(\"Value\"),\", \").getItem(1).alias(\"StateCode\")\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Verify the split worked as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_airports_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Clean states__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_metadata_dfs['i94addrl'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_states_df = sas_metadata_dfs['i94addrl'].withColumn('Code', F.regexp_extract(F.col('Code'), '([A-Za-z\\d]{2}+)', 1)) \\\n",
    "                    .withColumn('Value', F.regexp_extract(F.col('Value'), '([a-zA-Z0-9\\.\\s\\-]+[a-zA-Z0-9\\.\\)])', 1))\n",
    "sas_states_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Clean modes of arrival__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_metadata_dfs['i94model'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_modes_df = sas_metadata_dfs['i94model'].withColumn('Code', F.regexp_extract(F.col('Code'), '(\\d{1})', 1)) \\\n",
    "                    .withColumn('Value', F.regexp_extract(F.col('Value'), '([a-zA-Z0-9\\s]+[a-zA-Z0-9])', 1))\n",
    "sas_modes_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Clean visa type__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_metadata_dfs['i94visa'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_visa_types_df = sas_metadata_dfs['i94visa'].withColumn('Code', F.regexp_extract(F.col('Code'), '(\\d{1})', 1)) \\\n",
    "                    .withColumn('Value', F.regexp_extract(F.col('Value'), '([a-zA-Z0-9]+)', 1))\n",
    "sas_visa_types_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Join the metadata with immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_countries_df.createOrReplaceTempView(\"SAS_Countries\")\n",
    "sas_airports_df.createOrReplaceTempView(\"SAS_Airports\")\n",
    "sas_states_df.createOrReplaceTempView(\"SAS_States\")\n",
    "sas_visa_types_df.createOrReplaceTempView(\"SAS_VisaTypes\")\n",
    "sas_modes_df.createOrReplaceTempView(\"SAS_Modes\")\n",
    "df_spark_clean_no_na.createOrReplaceTempView(\"Immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_clean = spark.sql(\"\"\"\n",
    "    SELECT cicid, i94yr, i94mon, i94port, \n",
    "        M.Value as mode, i94addr,\n",
    "        V.Value as visa_code,\n",
    "        C1.Value as i94_cit_country,\n",
    "        C2.Value as i94_res_country,\n",
    "        i94bir, biryear, gender, \n",
    "        visatype, arrival_date, departure_date\n",
    "    FROM Immigration I \n",
    "    INNER JOIN SAS_Countries C1 on I.i94cit = C1.Code\n",
    "    INNER JOIN SAS_Countries C2 on I.i94res = C2.Code \n",
    "    INNER JOIN SAS_VisaTypes V on I.i94visa = V.Code\n",
    "    INNER JOIN SAS_Modes M on I.i94mode = M.Code\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Save cleaned immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_clean = df_immigration_clean \\\n",
    "    .withColumn('year_part', F.col('i94yr')) \\\n",
    "    .withColumn('month_part', F.col('i94mon')) \\\n",
    "    .coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_clean \\\n",
    "    .write \\\n",
    "    .partitionBy(\"year_part\", \"month_part\") \\\n",
    "    .save(path=DATALAKE_CLEAN_PATH+'/immigration', source='parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "In this project we're building data pipeline for analysing immigration flow to the US, and there's one data set containing this data. Other data sets contain complimentary data. This fits well into star schema with Immigration table as a fact, and data from other data sets as dimensions. Therefore, the model is as follows:\n",
    "\n",
    "##### Fact Table\n",
    "- __F_Immigration__ - immigration data.\n",
    "Columns: _cicid_, _year_, _month_, _residence_country_, _citizenship_country, _arrival_date_, _departure_date_, _arrival_mode_, _arrival_port_, _age_, _visa_type_, _birth_year_, _visa_code_, _gender_, _arrival_state_\n",
    "\n",
    "##### Dimension Tables\n",
    "- __D_Airport__ - U.S. aiports. \n",
    "Columns: _identifier_, _type_, _airport_name_, _municipality_, _iata_code_, _state_code_\n",
    "- __D_Demographics__ - U.S. demographics by state.\n",
    "Columns: _city_, _state_, _median_age_, _male_population_, _female_population_, _total_population_, _num_veterans_, _foreign_born_, _avg_household_size_, _state_code_, _race_\n",
    "- __D_US_State__ - U.S. states - abbreviations and full name.\n",
    "Columns: _state_code_, _state_name_\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Here are the steps necessary to pipeline the data into the data model:\n",
    "1. Load data from files (SAS data from parquet, demographics, airport and SAS metadata - from csv) as save to S3 (raw datalake)\n",
    "2. Read raw data from datalake, clean it and save back to S3 (conformed layer).\n",
    "3. Build data model (using snowflake schema, described above) from cleaned data and save back to S3 (application layer).\n",
    "4. Load data from application layer into Redshift for analysis.\n",
    "5. Do data quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "We need to load cleaned data from S3 which was saved there in previous step but since we've already loaded all the data earlier, for simplicity we won't be re-loading it again from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "generate_unique_id = F.udf(lambda : str(uuid.uuid4()),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_dataframe_to_s3_as_json(df, dest_path):\n",
    "    df.write.option(\"header\", True) \\\n",
    "        .mode('overwrite') \\\n",
    "        .json(dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create Immigration fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "f_immigration = df_immigration_clean.withColumn(\"immigration_key\", generate_unique_id()) \\\n",
    "                        .select(F.col('immigration_key'), \\\n",
    "                        F.col('cicid'), \\\n",
    "                        F.col('i94yr').alias('year'), \\\n",
    "                        F.col('i94mon').alias('month'), \\\n",
    "                        F.col('i94_cit_country').alias('citizenship_country'),\\\n",
    "                        F.col('i94_res_country').alias('residence_country'), \\\n",
    "                        F.col('visa_code').alias('visa_category'), \\\n",
    "                        F.col('visatype').alias('visa_type'), \\\n",
    "                        F.col('arrival_date'), \\\n",
    "                        F.col('departure_date'), \\\n",
    "                        F.col('mode').alias('arrival_mode'), \\\n",
    "                        F.col('i94port').alias('arrival_port'), \\\n",
    "                        F.col('i94bir').alias('age'), \\\n",
    "                        F.col('biryear').alias('birth_year'), \\\n",
    "                        F.col('gender'), \\\n",
    "                        F.col('i94addr').alias('arrival_state'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Save Immigration table to S3 (application layer, to be loaded into Redshift for analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_dataframe_to_s3_as_json(f_immigration, DATALAKE_APPLICATION_PATH+'/f_immigration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create Demographics dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics_data_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_demographics = df_demographics_data_clean.withColumn(\"demographics_key\", generate_unique_id()) \\\n",
    "                        .select(F.col('demographics_key'), \\\n",
    "                        F.col('City').alias('city'), \\\n",
    "                        F.col('State').alias('state'), \\\n",
    "                        F.col('Median_Age').alias('median_age'),\\\n",
    "                        F.col('Male_Population').alias('male_population'), \\\n",
    "                        F.col('Female_Population').alias('female_population'), \\\n",
    "                        F.col('Total_Population').alias('total_population'), \\\n",
    "                        F.col('Number_of_Veterans').alias('num_veterans'), \\\n",
    "                        F.col('Foreign-born').alias('foreign_born'), \\\n",
    "                        F.col('Average_Household_Size').alias('avg_household_size'), \\\n",
    "                        F.col('State_Code').alias('state_code'), \\\n",
    "                        F.col('Race').alias('race'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_dataframe_to_s3_as_json(dim_demographics, DATALAKE_APPLICATION_PATH+'/d_demographics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create Airports dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Rename columns and select only columns that are needed for the model\n",
    "dim_airports = df_airport_codes_data_clean.withColumn(\"airport_key\", generate_unique_id()) \\\n",
    "                    .select(F.col('airport_key'), \\\n",
    "                    F.col('ident').alias('identifier'), \\\n",
    "                    F.col('type').alias('type'), \\\n",
    "                    F.col('name').alias('airport_name'),\\\n",
    "                    F.col('state_code'), \\\n",
    "                    F.col('municipality'), \\\n",
    "                    F.col('iata_code'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_dataframe_to_s3_as_json(dim_airports, DATALAKE_APPLICATION_PATH+'/d_airport')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create States dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_airports.createOrReplaceTempView(\"Airports\")\n",
    "dim_demographics.createOrReplaceTempView(\"Demographics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_states = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT a.state_code, \n",
    "        d.state as state_name\n",
    "    FROM Airports A \n",
    "    LEFT JOIN Demographics D on D.state_code = A.state_code\n",
    "    ORDER BY A.state_code\n",
    "    \"\"\")\n",
    "dim_states.show(51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As we can see, 3 states don't have corresponding state names. Let's fill them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_states_updated = spark.createDataFrame([\n",
    "    ('VT', \"Vermont\"),\n",
    "    ('WV', \"West Virginia\"),\n",
    "    ('WY', \"Wyoming\")\n",
    "], (\"state_code\", \"state_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_states.createOrReplaceTempView(\"US_States\")\n",
    "states_without_null_names = spark.sql(f\"SELECT * FROM US_States WHERE state_name is not NULL\")\n",
    "dim_states_final = states_without_null_names.union(dim_states_updated)\n",
    "dim_states_final.head(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_states = dim_states_final.withColumn(\"state_key\", generate_unique_id()) \\\n",
    "                            .select(F.col('state_key'), F.col('state_code'), F.col('state_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_dataframe_to_s3_as_json(dim_states, DATALAKE_APPLICATION_PATH+'/d_us_state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Load all tables into Redshift\n",
    "Script for creating Redshift tables is in create_tables.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "IAM_ROLE = config.get('AWS', 'IAM_ROLE')\n",
    "DWH_ENDPOINT = config.get('CLUSTER', 'DWH_ENDPOINT')\n",
    "DWH_USER = config.get('CLUSTER', 'DWH_USER')\n",
    "DWH_PASSWORD = config.get('CLUSTER', 'DWH_PASSWORD')\n",
    "DWH_PORT = config.get('CLUSTER', 'DWH_PORT')\n",
    "DWH_DB = config.get('CLUSTER', 'DWH_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "table_names = ['d_airport', 'd_demographics', 'd_us_state', 'f_immigration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "copy_statement = \"\"\"\n",
    "                    COPY {} FROM '{}' \n",
    "                    IAM_ROLE '{}'\n",
    "                    format as json 'auto';\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_USER, DWH_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for table in table_names:\n",
    "    try:\n",
    "        copy_data = copy_statement.format(table, \\\n",
    "                               f\"{DATALAKE_APPLICATION_PATH.replace('s3a', 's3')}/{table}/part-\", \\\n",
    "                               IAM_ROLE)\n",
    "        %sql $copy_data\n",
    "    except Exception as ex:\n",
    "        print(f'Error while loading data for table {table}: {str(ex)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "The data quality checks that will be performed:\n",
    "- Number of rows in source and destination. By doing this check, we automatically check that destination tables have data, so no need to check separately for that.\n",
    "   \n",
    "Also, no need to check that non-null columns don't have null values because this is by design (see script that creates tables).\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(dbname=DWH_DB, host=DWH_ENDPOINT, port=DWH_PORT, user=DWH_USER, password=DWH_PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def validate_counts(conn, table_name, expected_count):\n",
    "    \"\"\"\n",
    "    Checks the number of records in table is same as expected\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f\"select count(*) from {table_name};\")\n",
    "        data = np.array(cur.fetchone())\n",
    "        cur.close()\n",
    "        record_count = data[0]\n",
    "        if data and int(data[0]) == expected_count:\n",
    "            return record_count\n",
    "        raise ValueError(f'Data quality check (row number) for table {table_name} failed. Expected rows: {expected_count} Actual: {record_count}')\n",
    "    except Exception as ex:\n",
    "        print(f'Exception while checking number of rows in table {table_name}: {str(ex)}')\n",
    "        cur.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Get the number of rows in each table before it was loaded into Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tables_with_counts = {'d_airport': dim_airports.count(), \\\n",
    "          'd_demographics': dim_demographics.count(),\\\n",
    "          'd_us_state': dim_states.count(), \\\n",
    "          'f_immigration': f_immigration.count()}\n",
    "tables_with_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Verify the number of rows in Redshift table is the same as in source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "counts = [validate_counts(conn, tbl, cnt) for tbl, cnt in tables_with_counts.items()]\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "See Data dictionary in file _data_dictionary.txt_.\n",
    "Data model is in _US-Immigration-ER-Diagram.png_ file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "Since files in the main data set are large Spark is used to load the data. S3 was selected for data storage as easy-to-use and inexpensive solution. Redshift was selected as the tool for data analysis because it seamlessly integrates with the chosen data storage and it is also reliable and scalable.\n",
    "\n",
    "Frequency of data updates: immigration data changes every day, but trends don't change that often, so updating data once a month should be enough for the goal of this project.\n",
    "\n",
    "If the requirements change, here's how the project would be implemented differently:\n",
    " * The data was increased by 100x.\n",
    "   This does not change data storage or loading because S3 can cope with such amount of data.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "   The pipeline would run in Airflow scheduled to run at 6am (so it definitely finishes by 7am).\n",
    " * The database needed to be accessed by 100+ people.\n",
    "   Redhift can support 100+ connections to this would not be a problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
